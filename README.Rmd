---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# fgwqsr -- Frequentist Grouped Weighted Quantile Sum Regression <img src="man/figures/FGWQSR_hex.png" width="100" align="right" />
Fully frequentist estimation and inference for Weighted Quantile Sum Regression, enabling inference on both individual and grouped pollutants without the need for data splitting. [Rud et al (2025)](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.70078).  

Also included in this package is an implementation of Bayesian Grouped Weighted Quantile Sum Regression (bgwqsr) introduced by Wheeler, David C et al. that fits Markov Chain Monte Carlo (MCMC) chains in parallel through the runjags package.  

<!-- badges: start -->
<!-- badges: end -->


# Overview 
<br>
FGWQSR estimates parameters to the following model: 

<br>


$$y_i \sim \text{Bernoulli}(\pi_i) $$

$$\text{logit}(\pi_i) = c_0+ \sum\limits_{g = 1}^G \gamma_g \bigg( \sum\limits_{k = 1} ^{c_g} w_{g,k} \cdot q_{g,k,i}\bigg) + \sum_{r = 1}^R \phi_rz_{r,i} $$

where 

* $y_i$ - outcome variable (coded 0 1)

* $c_0$ - offset variable (intercept)

* $G$ - total number of chemical mixture groups 

* $\gamma_g$ - group index effect corresponding to mixture group $g$

* $c_g$ - total number of chemicals in group $g$

* $w_{g,k}$ - weight for chemical $k$ in group $g$

* $q_{g,k,i}$ - quantized chemical exposure $k$ in group $g$ for individual $i$ 

* $R$ - total number of adjusting covariates 

* $\phi_r$ - effect of $r^{\text{th}}$ adjusting covariate 

* $z_{r,i}$ - measure of adjusting covariate $r$ for individual $i$


subject to the constraints $$\sum_{k = 1}^{c_g} w_{g,k} =1, \ w_{g,k} \in (0,1)$$ without requiring any data splitting

<br>



# Installation

You can install the development version of fgwqsr from [GitHub](https://github.com/Daniel-Rud/fgwqsr) with:

```{r}
# install.packages("devtools")
remotes::install_github("Daniel-Rud/fgwqsr")
```

```{r example}
library(fgwqsr)
```

The `fgwqsr` package depends on JAGS installation through the `runjags` package, which is used to fit BGWQSR models.  Please see the `runjags` installation instructions for your operating system [here](https://cran.r-project.org/web/packages/runjags/vignettes/Installation.html).  Note that JAGS is not required to fit FGWQSR models.

In order to use fgwqsr, let us first generate some data

# Data Generation 
<br>
To begin, let us generate some data to use.  We will have 14 underlying chemical exposure variables, with mixture groups of sizes (5,4,5) and weight distribution   

* $\mathbf{w}_1 = (1/3,1/3,1/3,0,0)$

* $\mathbf{w}_2 = (1/2,1/2, 0, 0)$

* $\mathbf{w}_3 = (1/3,1/3,1/3,0,0)$. 

We will use a fixed correlation structure such that the correlation of chemicals within a group is .7 and the cross correlations between 2 chemicals in different groups is .5. 
<br>
We first define ESS and CCS. ESS is a list that contains the the sample size and the true underlying weight distribution of the chemicals in each group.  CCS is a two element vector where the first index defines the within group correlation and the second index controls the between group correlation.  The sample size for our dataset is fixed to $n = 10,000$ and we set the distribution of the true underlying weights as follows: 

<br>
We set the group index effects for the three groups as follows (in OR scale): $\exp(\gamma_1) = .5$, $\exp(\gamma_2) = 1$, $\exp(\gamma_3) = 1.5$
```{r}
n = 10000

gamma = log(c(.5,1,1.5)) # group index sizes in log odds scale

ess = list(n = n, 
           weights = list(w1 = c(1/3,1/3,1/3,0,0), 
                          w2 = c(1/2,1/2, 0, 0), 
                          w3 = c(1/3,1/3,1/3,0,0)
           )
)

ccs = c(.5,.1)
```
<br>

We create a function to create the desired correlation matrix.  
```{r}
# function to create correlation matrix 
create_corr_mat = function(ESS, CCS)
{
  num_pollutants = length(unlist(ESS$weights))
  num_groups = length(ESS$weights)
  
  current_index = 1
  
  corr_mat = diag(num_pollutants)
  
  # populate within group correlation blocks 
  for(i in 1:num_groups) # for each of the pollutant groups
  {
    corr_mat[current_index:(current_index + length(ESS$weights[[i]])-1), 
             current_index:(current_index + length(ESS$weights[[i]])-1) ] = CCS[1]
    current_index = current_index + length(ESS$weights[[i]])
  }
  
  # populate between group pollutant correlations 
  # set diagonal to 1 
  for(i in 1:num_pollutants)
  {
    corr_mat[i, which(corr_mat[i,] == 0)] = CCS[2]
    corr_mat[i,i] = 1
  }
  
  return(corr_mat)
}

```

<br>

Now we create a correlation matrix

```{r}
corr = create_corr_mat(ESS = ess, CCS = ccs)

corr %>% data.frame # for viewing  
```

<br>

We generate our exposure data based on the derived correlation structure.  We will use 5 quantiles, as that is the default for FGWQSR.  
```{r}
set.seed(11) # for reproducibility
chem_data = mvtnorm::rmvnorm(n, mean = rep(0, 14), sigma = corr)
```
<br>

Next, we quantize the exposure variables in order to make comparisons between the true underlying oracle model and FGWQSR estimates.  Note that we will subtract 1 from our quantized variables, so that the first quintile corresponds to the measure of 0.  By doing this, we can ensure that we have an interpretable model intercept. For ease of understanding, we give the chemical variables arbitrary names of chemical constituents.   

```{r}
chem_data = apply(chem_data, MARGIN = 2, statar::xtile, n = 5) - 1
colnames(chem_data) = chemicals <- c(
  "PM2.5", 
  "NO2", 
  "O3", 
  "SO2", 
  "CO", 
  "Lead", 
  "Mercury", 
  "Arsenic", 
  "Cadmium", 
  "Benzene", 
  "Toluene", 
  "Phthalates", 
  "Bisphenol_A", 
  "Polychlorinated_Biphenyls"
)

head(chem_data %>% data.frame)
```

<br>

Finally, we create the logistic outcome variable 
```{r}

intercept = 0

# create logit(pi) outcome 
logit_pi =  gamma[1] * (chem_data[, 1:5] %*% ess$weights$w1) + 
  gamma[2] * (chem_data[, 6:9] %*% ess$weights$w2) + 
  gamma[3] * (chem_data[, 10:14] %*% ess$weights$w3) + intercept

# transform to pi 
pi = stats::plogis(logit_pi)

# transform to bernoulli outcome 
y = sapply(pi, FUN = function(p) rbinom(1,1,p))

# create dataset 
data = data.frame(y = y, chem_data)

# view dataset
head(data)
```
<br>
Now, we are ready to fit a FGWQSR model 

<br>

# Fitting using FGWQSR 
<br>
From the FGWQSR package, the main function we will be using to fit models if the `fgwqsr()` function.  The model formula that we specify will be different than traditional formulas in lm and glm, as we will need to denote our mixture groups.  Three special characters are used in fgwqsr formulas: 
<br>

* `|` - denotes the boundary of a mixture group, used to seperate chemicals within a mixture group. 

* `/` - denotes the end of the mixture group specification, adjusting covariates can be added to the formula after this character.  If no adjusting covariates, do not need to specify.

* `i.` - precedes categorical variables to denote a categorical variable.  For example, if we have a categorical variable cat_var, we would denote this in the model formula by i.cat_var.  This is similar to the stata syntax to declare categorical variables.  

<br>
The `fgwqsr()` function has other options too: 
<br>

* `data` - a data frame object containing variable columnames referenced in model formula (cat vars do not need to be named with i. in column names).

* `family` - can be one of 'binomial', 'gaussian' or 'poisson' for binary, continuous, and count outcomes respectivley.  

* `quantiles` - number of quantiles to quantize the exposure variables in the mixture portion of the model.

* `n_mvn_sims` - defines resolution for simulated null distribution for group index and single chemical LRTs.  Default is 10,000.   

* `zero_threshold_cutoff` - Value within (0,.5] that defines how often
parameters estimated close to the boundary of the parameter
space are assigned a boundary cone in the constrained multivariate normal
monte carlo inference procedure.  A \code{zero_tolerance_threshold} value of 0.5 will
assign parameters with FGWQSR maximum likelihood estimates of precisely 0 the
boundary cone while a \code{zero_tolerance_threshold} value of 0 will assign all
parameters a boundary cone.  Reasonable values may be within [0.05, 0.5] --
all choices of \code{zero_tolerance_threshold} are asymptotically equivalent.  The default is set to zero_tolerance_threshold = 0.5.

* `verbose` - Displays messages and progress bar while fitting FGWQSR model.  Default is TRUE.  

* `cores` - number of cores to parallelize on for fitting nested models and simulated null LRT distributions.  Default is number of available cores on user device. 

* `optim_control_list` - option to supply control options to optim.  

<br>

First, we will focus on fitting models with only chemical mixture variables.  To do this, we will supply the model formula as follows:

```{r}

mod_formula = y ~ PM2.5 + NO2 + O3 + SO2 + CO | Lead + Mercury + Arsenic + Cadmium | Benzene + Toluene + Phthalates + Bisphenol_A + Polychlorinated_Biphenyls


```
<br>

Notice that we did not use the `/` character since we did not include adjusting covariates.  Now, we can fit the model using the function `fgwqsr()`.
```{r}
fgwqsr_fit = fgwqsr(formula = mod_formula,
                    data = data,
                    quantiles = 5,
                    family = 'binomial',
                    n_mvn_sims = 10000,
                    verbose = T)

```
<br>

We can see the model summary using the call `summary()`

```{r}

summary(fgwqsr_fit)

```

<br>

We can plot a visual summary of the model output using `plot()`

```{r, fig.height = 6, fig.width = 14}

plot(fgwqsr_fit)

```
<br>
We can compare the true underlying group indices and chemical weights with their estimates, along with if the correct inference is made.  Note that dashes are inserted for the true weights of group 2, since the true underlying group index is null (thus, the weights are unidentifiable).

```{r}
group_index_frame = data.frame("True Group Index" = exp(gamma),
                               "Estimated Group Index" = exp(fgwqsr_fit$inference_frames$group_index_frame[,1]) %>% round(3),
                               "Signficiant?" = ifelse(fgwqsr_fit$inference_frames$group_index_frame[,3] < .05, "Yes", "No"), check.names = F)

rownames(group_index_frame) = rownames(fgwqsr_fit$inference_frames$group_index_frame)

true_weights = ess$weights %>% unlist %>% round(3)
true_weights[6:9] = "-"


weight_frame = data.frame("True Weight" = true_weights,
                          "Estimated Weight" = fgwqsr_fit$inference_frames$weight_frame[,1] %>% round(3),
                          "Signficiant?" = ifelse(fgwqsr_fit$inference_frames$weight_frame[,3] < .05, "Yes", "No"), check.names = F)

group_index_frame; weight_frame


```
<br>


# Fitting with Adjusting Continuous and Categorical Covariates

As mentioned earlier, the `/` character in the model formula sent to `fgwqsr()` indicates that adjusting covariates follow this character.  Continuous characters can be referenced with their columnname from the dataset, while categorical variables need to be referenced using the prefix `i.`.  We will illustrate this through an example where we create a continuous `weight` variable and a categorical `city` variable.  We will give an effect (in log odds scale) of .5 to weight, .2 to the comparison between San Bernardino and reference Los Angeles, and -1 to the comparison between Santa Barbara and Los Angeles.

<br>

```{r}
set.seed(11)
# create adjusting covariates
weight = rnorm(n = n, mean = 68, sd = 2.5)
city = sample(c("Los_Angeles", "San_Bernardino", "Santa_Barbara"), size = n, replace = T)

# need to adjust intercept for case control ratio
intercept = -3.3

# create logit(pi) outcome WITH adjusting covariates
logit_pi =  gamma[1] * (chem_data[, 1:5] %*% ess$weights$w1) +
  gamma[2] * (chem_data[, 6:9] %*% ess$weights$w2) +
  gamma[3] * (chem_data[, 10:14] %*% ess$weights$w3) +
  .05 *weight +
  .2*ifelse(city == "city_2", 1, 0) + -1*ifelse(city == "city_3", 1, 0) +
  intercept

# transform to pi

pi = stats::plogis(logit_pi)

# transform to bernoulli outcome
y = sapply(pi, FUN = function(p) rbinom(1,1,p))

# create dataset

data = data.frame(y = y, chem_data, weight = weight, city = city)

head(data)
```

<br>
Now, we specify the formula to include the continuous and categorical variables and call `fgwqsr()`.
```{r}

mod_formula_adj = y ~ PM2.5 + NO2 + O3 + SO2 + CO | Lead + Mercury + Arsenic + Cadmium | Benzene + Toluene + Phthalates + Bisphenol_A + Polychlorinated_Biphenyls / weight + i.city

fgwqsr_fit_adj = fgwqsr(formula = mod_formula_adj,
                        data = data,
                        quantiles = 5,
                        family = 'binomial',
                        n_mvn_sims = 10000,
                        verbose = T)

```
<br>
Again, we use the function `summary()` to view the results of the `fgwqsr()` call.

```{r}

summary(fgwqsr_fit_adj)

```
<br>

<br>

Plotting a visual summary of the model output using `plot()` ...

```{r, fig.height = 6, fig.width = 14}

plot(fgwqsr_fit)

```

Finally, we can compare the true parameter value with our estimates from FGWQSR.

```{r}

group_index_frame = data.frame("True Group Index" = exp(gamma),
                               "Estimated Group Index" = exp(fgwqsr_fit_adj$inference_frames$group_index_frame$Estimate) %>% round(3),
                               "Signficiant?" = ifelse(fgwqsr_fit_adj$inference_frames$group_index_frame$`P-value` < .05, "Yes", "No"), check.names = F)

rownames(group_index_frame) = rownames(fgwqsr_fit_adj$inference_frames$group_index_frame)

true_weights = ess$weights %>% unlist %>% round(3)
true_weights[6:9] = "-"


weight_frame = data.frame("True Weight" = true_weights,
                          "Estimated Weight" = fgwqsr_fit_adj$inference_frames$weight_frame$`Weight Estimate` %>% round(3),
                          "Signficiant?" = ifelse(fgwqsr_fit_adj$inference_frames$weight_frame$`P-value` < .05, "Yes", "No"), check.names = F)


adj_cov_frame = data.frame("True Covariate Effect" = c(-3.3, .5,.2,-1),
                           "Estimated Covariate Effect" = fgwqsr_fit_adj$inference_frames$adj_param_frame$Estimate,
                           "Significant?" = ifelse(fgwqsr_fit_adj$inference_frames$adj_param_frame$`P(Z > |z|)` < .05, "Yes", "No"), check.names = F)

rownames(adj_cov_frame) = rownames(fgwqsr_fit_adj$inference_frames$adj_param_frame)

group_index_frame; weight_frame; adj_cov_frame


```

# Fitting Models with BGWQSR

We also provide in this package functions to run BGWQSR models using the runjags package.  The `bgwqsr()` function takes a model formulation similar to that in `fgwqsr()`, also utilizing the special characters `|`, `/`, and `i.`.  Fitting the BGWQSR model using runjags allows us to leverage the "parallel" method in runjags that allows us to fit independent mcmc chains on multiple cores, speeding up fitting time.  The `bgwqsr()` function takes several arguments: 

* `formula`  A formula for model fitting of BGWQSR.  Please see description for formula construction

* `data` dataframe that contains all covariates and the outcome data.  Column names of dataframe should match those referenced int he model formula.

* `quantiles` number of quantiles to quantize the exposure variables in the mixture portion of the model.  Default value is 5.

* `n.iter` number of mcmc iterations after burnin and adapt iterations PER mcmc chain.

* `n.burnin` number of mcmc burnin samples PER mcmc chain

* `n.thin` thinning interval for mcmc samples PER mcmc chain

* `n.chains` number of separate independent mcmc chains to use.

* `n.adapt` number of mcmc samples to perform mcmc adaption PER mcmc chain.

* `inits` initial values to provide for prior distributions.

* `method` method for mcmc fitting, a passed argument to run.jags function.  Can be one of: `rjags`, `simple`, `interruptible`, `parallel`, `rjparallel`, `background`, `bgparallel`, or `snow`.

A note of **caution**: JAGS does not allow for variables with spaces in their names -- please name variables appropriately!

```{r}

bgwqsr_fit = bgwqsr(formula = mod_formula_adj,
                    data = data,
                    quantiles = 5,
                    n.iter = 5000,
                    n.adapt = 1000,
                    n.burnin = 4000,
                    n.thin = 1, 
                    n.chains = 3,
                    method = "parallel")

```

<br>

To retrieve the summary measures from the MCMC sampling, we can extract the summaries object from the bgwqsr model.

```{r}
bgwqsr_fit$model$summaries
```

We can analyze the mixing of the markov chains and corresponding autocorrelation plots using functions from the coda package.  We will
output only the first few plots.

```{r, fig.width = 10, fig.height = 10}
par(mfrow = c(3,3))
coda::traceplot(bgwqsr_fit$model$mcmc) # traceplot
```

```{r, fig.width = 10, fig.height = 10}
coda::autocorr.plot(bgwqsr_fit$model$mcmc, ask = F) # autocorrelation plot
```


```{r, fig.width = 10, fig.height = 10}
par(mfrow = c(3,3))
coda::densplot(bgwqsr_fit$model$mcmc) # posterior density plots
```

```{r, fig.width = 10, fig.height = 10}
plot(bgwqsr_fit$model$mcmc) # combines traceplot() and densplot()
```

Finally, we can look at plots for the posterior credible intervals for group indices, single chemical weights, and corresponding posterior means.  To do this, we can use the functions `plot_weights()`, `plot_betas()`, and `plot_result()`.  `plot_results()` combines both plots generates by plot_weights and plot_betas into a side by side figure.  Below are code examples for each of the three function calls.

```{r}
plot_betas(bgwqsr_fit)
```

```{r}
plot_weights(bgwqsr_fit)
```

```{r, fig.width = 12}
plot_results(bgwqsr_fit)
```

# Community Guidelines

We welcome contributions and engagement from the community to help improve and extend the functionality of `fgwqsr`. To maintain a collaborative and supportive environment, please follow these guidelines:

## Contributing
- Contributions of all kinds are welcome, including code improvements, documentation updates, bug fixes, and new feature proposals.  
- Before submitting a pull request, please open an issue to discuss your idea or bug report.  
- Ensure that code contributions include appropriate documentation and tests where applicable.  
- Follow the existing code style and structure to maintain consistency.  

## Reporting Issues
- If you encounter a problem, please [open an issue](https://github.com/Daniel-Rud/fgwqsr/issues) with a clear description of the bug or unexpected behavior.  
- Include reproducible examples (minimal code and sample data) whenever possible.  
- Clearly state your R version, operating system, and any other relevant session information (`sessionInfo()`).  

### Seeking Support
- For usage questions or clarification about methods, please first consult the package vignettes and documentation.  
- If further support is needed, feel free to open a discussion or issue.  


